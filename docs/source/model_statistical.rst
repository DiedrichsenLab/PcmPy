.. _model:

Statistical Model
-----------------
PCM is based on a generative model of the measured brain activity data **Y**, a matrix of *N x P* activity measurements, referring to *N* time points (or trials) and *P* voxels (or channels). The data can refer to the minimally preprocessed raw activity data, or to already deconvolved activity estimates, such as those obtained as beta weights from a first-level time series model. **U** is the matrix of true activity patterns (a number of conditions x number of voxels matrix) and **Z** the design matrix. Also influencing the data are effects of no interest **B** and noise:

.. math::
    \begin{array}{c}\mathbf{Y} = \mathbf{ZU+XB}+\epsilon\\
    \mathbf{u}_{p}  \sim N(\mathbf{0},\mathbf{G})\\
    \epsilon_p \sim N(\mathbf{0},\mathbf{S}\sigma^{2}) \end{array}

Assumption about the signal (**U**)
...................................
The activity profiles (:math:`\mathbf{u}_p` columns of **U**) are considered to be a random variable. Representational models therefore do not specify the exact activity profiles of specific voxels, but simply the characteristics of the distribution from which they originate. Said differently, PCM is not interested in which voxel has which activity profiles - it ignores their spatial arrangement. This makes sense considering that activity patterns can vary widely across different participants and do not directly impact what can be decoded from a region. For this, only the distribution of these activity profiles in this region is important.

PCM assumes is that the expected mean of the activity profiles is zero. If we are not interested in how much a voxel is activated, but only how acitivity differs between conditions, we would model the mean for each voxel using the fixed effects :math:`\mathbf{X}`. While one could also artificially remove the mean of each condition across voxels, this approach would remove differences that, from the persepctive of decoding and representation, are highly meaningful.

The third assumption is that the activity profiles come from a multivariate Gaussian distribution. This is likely the most controversial assumption, but it is motivated by a few reasons: First, for fMRI data the multi-variate Gaussian is often a relatively appropriate description. Secondly, the definition causes us to focus on the mean and covariance matrix, **G**, as sufficient statistics, as these completely determine the Gaussian. Thus, even if the true distribution of the activity profiles is better described by a non-Gaussian distribution, the focus on the second moment is sensible as it characterizes the linear decodability of any feature of the stimuli.

Assumptions about the Noise
...........................

Finally, we assume that the noise of each voxel is Gaussian with a temporal covariance that is known up to a constant term :math:`\sigma^{2}`. Given the many additive influences of various noise sources on fMRI signals, Gaussianity of the noise is, by the central limit theorem, most likely a very reasonable assumption, which is commonly made in the fMRI literature. The original formulation of PCM used a model which assumed that the noise is also temporally independent and identically distributed (i.i.d.) across different trials, i.e. :math:`\mathbf{S} = \mathbf{I}` . However, as pointed out recently [@RN3638], this assumption is often violated in non-random experimental designs with strong biasing consequences for estimates of the covariance matrix. If this is violated, we can either assume that we have a valid estimate of the true covariance structure of the noise (**S**), or we can model different parts of the noise structure (see section Noise assumptions).

The noise is assumed to come from a multivariate normal distribution with covariance matrix :math:`\mathbf{S}\sigma^{2}`. What is a reasonable noise structure to assume? First, the data can usually be assumed to be independent across imaging runs. If the data are regression estimates from a first-level model, and if the design of the experiment is balanced, then it is usually also permissible to make the assumption that the noise is independent within each imaging run :math:`\mathbf{S}=\mathbf{I}`, [@RN3033]. Usually, however, the regression coefficients from a single imaging run show positive correlations with each other. This is due to the fact that the regression weights measure the activation during a condition as compared to a resting baseline, and the resting baseline is common to all conditions within the run [@RN3033]. To account for this, it is recommended to remove the mean pattern across by modeling the run mean as a fixed effects. This can be done by setting the option `runEffect = 'fixed'` in the model fittting routines mentioned below.  This effectively accounts for any uniform correlation between activity estimates withn a run.

Usually, assuming equal correlations of the activation estimates within a run is only a rough approximation to the real co-varince structure. A better estimate can be obtained by using an estimate derived from the design matrix and the estimated temporal autocorrelation of the raw signal. As pointed out recently [@RN3638], the particular design can have substantial influence on the estimation of the second moment matrix. This is especially evident in cases where the design is such that the trial sequence is not random, but has an invariant structure (where trials of one condition are often to follow trials of another specific condition). The accuracy of our approximation hinges critically on the quality of our estimate of the temporal auto-covariance structure of the true noise. Note that it has been recently demonstrated that especially for high sampling rates, a simple autoregressive model of the noise is insufficient [@RN3550]. In all optimisation routine, a specific noise covariance structure can be specified by setting the option `'S'` in the model fitting routines to the NxN covariance estimate.

The last option is to estimate the covariance structure of the noise from the data itself. This can be achieved by introducing random effects into the generative model equation in section *Generative model*, which account for the covariance structure across the data. One example used here is to assume that the data are independent within each imaging run, but share an unknown covariance within each run, which is then estimated as a part of the covariance matrix [@RN3033]. While this approach is similar to just removing the run mean from the data as a fixed effect, it is a good strategy if we actually want to model the difference of each activation pattern against the resting baseline. When treating the mean activation pattern in each run as a random effect, the algorithm finds a compromise between how much of the shared pattern in each run to ascribe to the random run-to-run fluctuations, and how much to ascribe to a stable mean activation. This can be done by setting the option `runEffect = 'random'` in the model fitting routines. This approach is taken for example in `pcm_recipe_nonlinear` as here the resting baseline is of interest and should not be artificially removed.

Fourthly, the model assumes that different voxels are independent from each other. If we used raw data, this assumption would be clear violated, given the strong spatial correlation of noise processes in fMRI data. To reduce these dependencies we typically uses spatially pre-whitened data, which is divided by a estimate of the spatial covariance matrix [@RN3565; @RN3672]. One complication here is that spatial pre-whitening usually does not remove spatial dependencies completely, given the estimation error in the spatial covariance matrix [@RN3543].

Marginal likelihood
-------------------

When we fit a PCM model, we are not trying to estimate specific values of the the estimates of the true activity patterns **U**. This is a difference to encoding approaches, in which we would estimate the values of :math:`\mathbf{U}` by estimating the feature weights :math:`\mathbf{W}`. In PCM, we want to assess how likely the data is under any possible value of **U**, as specified by the prior distribution. Thus we wish to calculate the marginal likelihood

.. math::
    p\left(\mathbf{Y}|\theta\right)=\int p\left(\mathbf{Y}|\mathbf{U},\theta\right) p\left(\mathbf{U}|\theta\right) d\mathbf{U}.

This is the likelihood that is maximized in PCM in respect to the model parameters :math:`\theta`. For more details, see mathematical and algorithmic details.
